"""
Module d‚Äôinf√©rence combin√©e D-FINE + MobileSAM (Process C)
==========================================================

üí° Mise √† jour 20/10/2025 ‚Äî int√©gration du pr√©-processing ‚ÄúFSM mask-aware‚Äù
---------------------------------------------------------------------------
Ce module g√®re d√©sormais **les √©tapes 0 ‚Üí 3** du pipeline de visibilit√© :

    0. D√©tection globale via D-FINE (bbox_t, conf_t)
    1. Gating macro : si conf_t < œÑ_conf ‚Üí LOST direct
    2. Crop ROI autour de bbox_t
    3. Segmentation fine via MobileSAM
    4. Calcul des pond√©rations spatiales (W_edge, W_in, W_out)

Il constitue le moteur d‚Äôinf√©rence (Process C) entre la r√©ception GPU
et le module FSM (`core.state_machine.visibility_fsm`).

Flux typique :
    RawFrame (CPU) ‚Üí cpu_to_gpu.py ‚Üí GpuFrame (torch.Tensor)
    ‚Üí detection_and_engine.prepare_inference_inputs()
    ‚Üí visibility_fsm.evaluate_visibility()
    ‚Üí ResultPacket ‚Üí Gateway._outbox ‚Üí Slicer

R√¥le g√©n√©ral
------------
- charger et initialiser les mod√®les IA (D-FINE et MobileSAM)
- ex√©cuter l‚Äôinf√©rence D-FINE ‚Üí bbox/conf
- effectuer le crop ROI et la segmentation MobileSAM
- g√©n√©rer les pond√©rations spatiales utilis√©es pour les scores S‚ÇÅ‚ÄìS‚ÇÑ
- fournir un dictionnaire normalis√© pour le module FSM

Entr√©es attendues
-----------------
frame_tensor : image 2D √©chographique (numpy | torch)
model_paths  : dict avec au moins les cl√©s {"dfine", "mobilesam"}

Sortie type
-----------
{
    "state_hint": "VISIBLE" | "LOST",
    "bbox_t": <tuple[int,int,int,int]> | None,
    "conf_t": <float>,
    "roi": <ndarray> | None,
    "mask_t": <ndarray> | None,
    "W_edge": <ndarray> | None,
    "W_in": <ndarray> | None,
    "W_out": <ndarray> | None,
}

Impl√©mentations √† pr√©voir
-------------------------
- initialize_models()        ‚Üí chargement GPU des deux mod√®les
- run_detection()            ‚Üí inf√©rence D-FINE (bbox/conf)
- run_segmentation()         ‚Üí inf√©rence MobileSAM
- compute_mask_weights()     ‚Üí calcul morphologique des pond√©rations
- prepare_inference_inputs() ‚Üí orchestration globale (√©tapes 0 ‚Üí 3)

Le reste du Process C (run_inference/fuse_outputs/process_inference_once)
reste valide pour la compatibilit√© pipeline et KPI.
"""

from typing import Any, Dict, Tuple, Optional
import logging
import numpy as np
import time

from core.types import GpuFrame, ResultPacket
from core.queues.buffers import get_queue_gpu, get_queue_out, try_dequeue, enqueue_nowait_out

LOG = logging.getLogger("igt.inference")
LOG_KPI = logging.getLogger("igt.kpi")


# ============================================================
# 1. Chargement et initialisation des mod√®les
# ============================================================

def initialize_models(model_paths: Dict[str, str], device: str = "cuda") -> Dict[str, Any]:
    """Charge et initialise les mod√®les D-FINE et MobileSAM.

    Args:
        model_paths: dictionnaire contenant les chemins {'dfine': ..., 'mobilesam': ...}
        device: 'cuda' ou 'cpu'

    Returns:
        dict {'dfine': model_dfine, 'mobilesam': model_sam}
    """
    raise NotImplementedError


# ============================================================
# 2. √âtapes d‚Äôinf√©rence mask-aware
# ============================================================

def run_detection(dfine_model: Any, frame_tensor: Any) -> Tuple[Tuple[int, int, int, int], float]:
    """Ex√©cute le mod√®le D-FINE et renvoie (bbox_t, conf_t)."""
    raise NotImplementedError


def run_segmentation(sam_model: Any, roi: np.ndarray) -> Optional[np.ndarray]:
    """Ex√©cute MobileSAM sur la ROI et retourne le mask binaire."""
    raise NotImplementedError


def compute_mask_weights(mask_t: np.ndarray, width_edge: int = 3, width_out: int = 5) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Construit les trois pond√©rations spatiales W_edge, W_in, W_out √† partir du mask."""
    raise NotImplementedError


def prepare_inference_inputs(frame_t: np.ndarray, dfine_model: Any, sam_model: Any, tau_conf: float = 0.5) -> Dict[str, Any]:
    """
    Orchestration compl√®te des √©tapes 0 ‚Üí 3.

    0. Passe l‚Äôimage dans D-FINE ‚Üí bbox/conf.
    1. Si conf < œÑ_conf ‚Üí renvoie state_hint='LOST'.
    2. Crop ROI autour de la bbox.
    3. Passe la ROI dans MobileSAM.
    4. Calcule les pond√©rations spatiales (W_edge/W_in/W_out).

    Returns:
        dictionnaire pr√™t pour visibility_fsm.evaluate_visibility().
    """
    raise NotImplementedError


# ============================================================
# 3. Compatibilit√© Process C (inf√©rence g√©n√©rique)
# ============================================================

def run_inference(frame_tensor: GpuFrame, stream_infer: Any = None) -> Tuple[ResultPacket, float]:
    """Ex√©cute (mock) l‚Äôinf√©rence GPU et retourne un ResultPacket minimal."""
    raise NotImplementedError


def fuse_outputs(mask: Any, score: float, state: str) -> ResultPacket:
    """Fusionne les sorties et renvoie un ResultPacket standardis√©."""
    raise NotImplementedError


# ============================================================
# 4. Routine de boucle unique (mock actuelle)
# ============================================================

def process_inference_once(models: Any = None) -> None:
    """Consomme une GpuFrame, ex√©cute une inf√©rence (mock) et place le r√©sultat en sortie."""
    q_gpu = get_queue_gpu()
    gf = try_dequeue(q_gpu)
    if gf is None:
        return

    if LOG.isEnabledFor(logging.DEBUG):
        fid = getattr(getattr(gf, "meta", None), "frame_id", None)
        LOG.debug("Dequeued GpuFrame for inference: %s", fid)

    # Simulation minimale d‚Äôinf√©rence
    t0 = time.time()
    result: ResultPacket = {
        "frame_id": getattr(getattr(gf, "meta", None), "frame_id", None),
        "mask": None,
        "score": 1.0,
        "state": "OK",
        "timestamp": getattr(getattr(gf, "meta", None), "ts", None),
    }  # type: ignore[assignment]
    t1 = time.time()
    latency_ms = (t1 - t0) * 1000.0

    try:
        from core.monitoring.kpi import safe_log_kpi, format_kpi
        msg = format_kpi({"ts": t1, "event": "infer_event", "frame": result.get("frame_id"), "latency_ms": f"{latency_ms:.1f}"})
        safe_log_kpi(msg)
    except Exception:
        LOG.debug("Failed to emit KPI infer_event")

    try:
        q_out = get_queue_out()
        ok = enqueue_nowait_out(q_out, result)  # type: ignore[arg-type]
        if not ok:
            LOG.warning("Out queue full, result for frame %s dropped", result.get("frame_id"))
    except Exception as e:
        LOG.exception("Failed to enqueue result: %s", e)
