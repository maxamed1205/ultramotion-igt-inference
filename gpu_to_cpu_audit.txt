🔍 GPU→CPU TRANSFER AUDIT REPORT
==================================================

🔴 Critical (Combined) src\core\inference\dfine_infer.py:145 [to_cpu]
   frame_cpu = frame_mono.detach().to("cpu", non_blocking=False)

🔴 Critical (Combined) src\core\inference\dfine_infer.py:145 [detach]
   frame_cpu = frame_mono.detach().to("cpu", non_blocking=False)

🟡 Medium (Review) src\core\inference\dfine_infer.py:173 [detach]
   conf = float(scores[idx].detach().item())

🟡 Medium (Review) src\core\inference\dfine_infer.py:173 [item]
   conf = float(scores[idx].detach().item())

🟡 Medium (Review) src\core\inference\dfine_infer.py:176 [detach]
   box_xyxy = boxes_xyxy[idx].detach()

🟡 Medium (Review) src\core\inference\dfine_infer.py:178 [cpu]
   return box_xyxy.to(dtype=torch.float32).cpu().numpy(), conf

🟠 Medium (Minor) src\core\inference\dfine_infer.py:178 [numpy]
   return box_xyxy.to(dtype=torch.float32).cpu().numpy(), conf

🟡 Medium (Review) src\core\inference\dfine_infer.py:222 [detach]
   conf = float(scores[idx].detach().item())

🟡 Medium (Review) src\core\inference\dfine_infer.py:222 [item]
   conf = float(scores[idx].detach().item())

🟡 Medium (Review) src\core\inference\dfine_infer.py:227 [detach]
   b = boxes[idx].detach()  # [cx, cy, w, h] in range [0, 1]

🟡 Medium (Review) src\core\inference\dfine_infer.py:243 [cpu]
   return box_xyxy.to(dtype=torch.float32).cpu().numpy(), conf

🟠 Medium (Minor) src\core\inference\dfine_infer.py:243 [numpy]
   return box_xyxy.to(dtype=torch.float32).cpu().numpy(), conf

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:99 [detach]
   ious = torch.diag(ious).detach()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:115 [detach]
   pred_score = F.sigmoid(src_logits).detach()

🟡 Necessary (Output) src\core\inference\d_fine\dfine_criterion.py:155 [detach]
   ref_points = outputs["ref_points"][idx].detach()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:183 [detach]
   weight_targets = ious.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:210 [detach]
   weight_targets_local.unsqueeze(-1).repeat(1, 1, 4).reshape(-1).detach()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:219 [detach]
   F.softmax(target_corners.detach() / T, dim=1),

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:266 [item]
   row_idx, col_idx = idx[0].item(), idx[1].item()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:321 [item]
   num_boxes_go = torch.clamp(num_boxes_go / get_world_size(), min=1).item()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:332 [item]
   num_boxes = torch.clamp(num_boxes / get_world_size(), min=1).item()

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:455 [detach]
   box_cxcywh_to_xyxy(src_boxes.detach()), box_cxcywh_to_xyxy(target_boxes)

🟡 Medium (Review) src\core\inference\d_fine\dfine_criterion.py:461 [detach]
   box_cxcywh_to_xyxy(src_boxes.detach()), box_cxcywh_to_xyxy(target_boxes)

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:89 [item]
   best_query = target_to_query[target_idx].item()

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:99 [item]
   if query_idx.item() not in used_queries:

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:100 [item]
   query_indices.append(query_idx.item())

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:102 [item]
   used_queries.add(query_idx.item())

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:185 [cpu]
   C_cpu = C.cpu()

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:197 [cpu]
   indices_pre = [(i.cpu().numpy(), j.cpu().numpy()) for i, j in indices]

🟠 Medium (Minor) src\core\inference\d_fine\matcher.py:197 [numpy]
   indices_pre = [(i.cpu().numpy(), j.cpu().numpy()) for i, j in indices]

🟡 Medium (Review) src\core\inference\d_fine\matcher.py:209 [cpu]
   C_cpu = C.cpu() if hasattr(C, 'cpu') else C

🟡 Necessary (Output) src\core\inference\d_fine\arch\dfine_decoder.py:415 [detach]
   output_detach = output.detach()

🟡 Medium (Review) src\core\inference\d_fine\arch\dfine_decoder.py:425 [detach]
   ref_points_initial = pre_bboxes.detach()

🟡 Medium (Review) src\core\inference\d_fine\arch\dfine_decoder.py:446 [detach]
   ref_points_detach = inter_ref_bbox.detach()

🟡 Necessary (Output) src\core\inference\d_fine\arch\dfine_decoder.py:447 [detach]
   output_detach = output.detach()

🟡 Medium (Review) src\core\inference\d_fine\arch\dfine_decoder.py:792 [detach]
   content = enc_topk_memory.detach()

🟡 Medium (Review) src\core\inference\d_fine\arch\dfine_decoder.py:794 [detach]
   enc_topk_bbox_unact = enc_topk_bbox_unact.detach()

🟡 Medium (Review) src\core\inference\d_fine\arch\utils.py:162 [item]
   upper_bound1 = (abs(up[0]) * abs(reg_scale)).item()

🟡 Medium (Review) src\core\inference\d_fine\arch\utils.py:163 [item]
   upper_bound2 = (abs(up[0]) * abs(reg_scale) * 2).item()

🟡 Medium (Review) src\core\inference\d_fine\arch\utils.py:354 [detach]
   return four_lens.reshape(-1).detach(), weight_right.detach(), weight_left.detach()

🔴 Critical (Tensor) src\core\inference\engine\inference_sam.py:235 [cpu]
   return mask.detach()  # reste sur GPU (pas de .cpu())

🔴 Critical (Combined) src\core\inference\engine\inference_sam.py:235 [detach]
   return mask.detach()  # reste sur GPU (pas de .cpu())

🔴 Critical (Tensor) src\core\inference\engine\inference_sam.py:239 [cpu]
   m = mask.detach().cpu().numpy()

🔴 Critical (Combined) src\core\inference\engine\inference_sam.py:239 [numpy]
   m = mask.detach().cpu().numpy()

🔴 Critical (Combined) src\core\inference\engine\inference_sam.py:239 [detach]
   m = mask.detach().cpu().numpy()

🟡 Medium (Review) src\core\inference\engine\model_loader.py:306 [to_cpu]
   dfine = dfine.to("cpu") if hasattr(dfine, "to") else dfine

🟡 Medium (Review) src\core\inference\engine\model_loader.py:307 [to_cpu]
   sam = sam.to("cpu") if hasattr(sam, "to") else sam

🔴 Critical (Combined) src\core\inference\engine\orchestrator.py:121 [cpu]
   arr_np = arr[0].permute(1, 2, 0).detach().cpu().numpy()

🟠 Medium (Minor) src\core\inference\engine\orchestrator.py:121 [numpy]
   arr_np = arr[0].permute(1, 2, 0).detach().cpu().numpy()

🔴 Critical (Combined) src\core\inference\engine\orchestrator.py:121 [detach]
   arr_np = arr[0].permute(1, 2, 0).detach().cpu().numpy()

🟡 Medium (Review) src\core\inference\MobileSAM\mobile_sam\automatic_mask_generator.py:188 [item]
   "predicted_iou": mask_data["iou_preds"][idx].item(),

🟡 Medium (Review) src\core\inference\MobileSAM\mobile_sam\automatic_mask_generator.py:190 [item]
   "stability_score": mask_data["stability_score"][idx].item(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:195 [cpu]
   m.detach().cpu().numpy(),

🟠 Medium (Minor) src\core\inference\MobileSAM\mobile_sam\predictor.py:195 [numpy]
   m.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:195 [detach]
   m.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:196 [cpu]
   iou.detach().cpu().numpy(),

🟠 Medium (Minor) src\core\inference\MobileSAM\mobile_sam\predictor.py:196 [numpy]
   iou.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:196 [detach]
   iou.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:197 [cpu]
   low.detach().cpu().numpy(),

🟠 Medium (Minor) src\core\inference\MobileSAM\mobile_sam\predictor.py:197 [numpy]
   low.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:197 [detach]
   low.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:202 [cpu]
   m.detach().cpu().numpy(),

🟠 Medium (Minor) src\core\inference\MobileSAM\mobile_sam\predictor.py:202 [numpy]
   m.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:202 [detach]
   m.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:203 [cpu]
   iou.detach().cpu().numpy(),

🟠 Medium (Minor) src\core\inference\MobileSAM\mobile_sam\predictor.py:203 [numpy]
   iou.detach().cpu().numpy(),

🔴 Critical (Combined) src\core\inference\MobileSAM\mobile_sam\predictor.py:203 [detach]
   iou.detach().cpu().numpy(),

🟡 Medium (Review) src\core\inference\MobileSAM\mobile_sam\Modeling\tiny_vit_sam.py:496 [item]
   dpr = [x.item() for x in torch.linspace(0, drop_path_rate,

🟠 Medium (Minor) src\core\preprocessing\cpu_to_gpu.py:420 [numpy]
   buf_np = buf.numpy()

